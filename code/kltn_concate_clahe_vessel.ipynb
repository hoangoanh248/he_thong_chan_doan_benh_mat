{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"kltn_concate_clahe_vessel.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"T93iOJrD0OYe","colab_type":"text"},"source":["**CÀI ĐẶT VÀ IMPORT THƯ VIỆN**"]},{"cell_type":"code","metadata":{"id":"c9DZNecG0SHi","colab_type":"code","colab":{}},"source":["!pip install catalyst==20.2.4\n","!pip install tqdm==4.33\n","!pip install efficientnet_pytorch\n","!pip install pytorch_toolbelt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ga52-VgD2IMb","colab_type":"code","colab":{}},"source":["from sklearn import svm\n","from keras.callbacks import CSVLogger\n","from keras.callbacks import LambdaCallback\n","from utils.utils import *\n","import pandas as pd\n","import sys\n","import os\n","import cv2\n","from keras.models import Sequential\n","from keras.layers import Dense\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from numpy import asarray\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.preprocessing.image import load_img\n","from keras.preprocessing.image import img_to_array\n","from keras.callbacks import LambdaCallback\n","from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n","from torch.utils.data import Dataset, DataLoader\n","import tensorflow as tf\n","import torch\n","import collections\n","from pytorch_toolbelt.inference import tta\n","from catalyst.dl.callbacks import InferCallback\n","from catalyst.dl.runner import SupervisedRunner\n","from torch.nn.functional import softmax\n","from catalyst.dl.callbacks import EarlyStoppingCallback, AccuracyCallback, F1ScoreCallback, ConfusionMatrixCallback, MixupCallback\n","from catalyst.contrib.nn.schedulers import OneCycleLR, ReduceLROnPlateau, StepLR, MultiStepLR\n","from torchvision import transforms\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torchsummary import summary\n","import pickle\n","import time\n","import copy\n","import sklearn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GhijjLxv2LT_","colab_type":"code","colab":{}},"source":["data = pd.read_csv('./utils/splited_train_new.csv')\n","splits = pickle.load(open('./utils/cv_split.pickle', 'rb'))\n","labels = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']\n","image_size = 240\n","fold_idx = 3\n","batch_size = 1\n","lr = 1e-5"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lY_TV1dO0WMz","colab_type":"text"},"source":["**TRAIN**"]},{"cell_type":"code","metadata":{"id":"z-XeGngG2Q7N","colab_type":"code","colab":{}},"source":["train_path1 = './data/clahe-train/'\n","valid_path1 = './data/clahe-train/'\n","\n","train_dataset1 = EyeDataset(dataset_path=train_path1,\n","                           labels=data.loc[splits['train_idx'][fold_idx],labels].values,\n","                           ids=data.loc[splits['train_idx'][fold_idx], 'id'].values,\n","                           albumentations_tr=aug_train_heavy(image_size))\n","\n","train_loader1 = DataLoader(train_dataset1,\n","                          num_workers=8,\n","                          pin_memory=False,\n","                          batch_size=batch_size,\n","                          shuffle=False)\n","\n","\n","logdir = './logs/'\n","modelA = prepare_model('efficientnet-b3', 8)\n","modelA.cuda()\n","modelA.load_state_dict(torch.load(os.path.join(logdir, './logs/clahe.pth'))['model_state_dict'])\n","modelA.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0VsAEmp52ZZI","colab_type":"code","colab":{}},"source":["train_path2 = './data/vessel-train/'\n","valid_path2 = './data/vessel-train/'\n","\n","train_dataset2 = EyeDataset(dataset_path=train_path2,\n","                           labels=data.loc[splits['train_idx'][fold_idx], labels].values,\n","                           ids=data.loc[splits['train_idx'][fold_idx], 'id'].values,\n","                           albumentations_tr=aug_train_heavy(image_size))\n","\n","train_loader2 = DataLoader(train_dataset2,\n","                          num_workers=8,\n","                          pin_memory=False,\n","                          batch_size=batch_size,\n","                          shuffle=False)\n","\n","logdir = './logs/'\n","modelB = prepare_model('efficientnet-b3', 8)\n","modelB.cuda()\n","modelB.load_state_dict(torch.load(os.path.join(logdir, './logs/vessel.pth'))['model_state_dict'])\n","modelB.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qcnmJL4B2roE","colab_type":"code","colab":{}},"source":["class NeuralNet(nn.Module):\n","    def __init__(self, modelA, modelB, nb_classes=8):\n","        super(NeuralNet, self).__init__()\n","        self.modelA = modelA\n","        self.modelB = modelB\n","        self.drop = nn.Dropout(0.3)\n","        self.out2 = nn.Linear(1536, 8)\n","        self.modelA.fc = nn.Identity()\n","        self.modelB.fc = nn.Identity()\n","\n","    def forward(self, x1, x2):\n","        x1 = self.modelA.extract_features(x1)\n","        x2 = self.modelB.extract_features(x2)\n","        \n","        x1 = nn.AdaptiveAvgPool2d((1, 1))(x1)\n","        x2 = nn.AdaptiveAvgPool2d((1, 1))(x2)\n","        x1 = x1.view(1,1536)\n","        x2 = x2.view(1,1536)\n","        x = x1+x2    \n","        sigmoid = torch.sigmoid(x)\n","        multi = torch.mul(sigmoid,x2)\n","        x = multi + x1\n","        x = self.drop(x)\n","        x = x.flatten()\n","        x = self.out2(x)\n","        return x\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5OvC181A20lb","colab_type":"code","colab":{}},"source":["x_model = NeuralNet(modelA, modelB)\n","x_model.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lR_7bW8M21CO","colab_type":"code","colab":{}},"source":["optimizer = torch.optim.Adam(x_model.parameters(), lr=1e-5)\n","criterion = nn.BCEWithLogitsLoss()\n","scheduler = ReduceLROnPlateau(optimizer=optimizer, factor=0.5, patience=5)\n","\n","PATH_SAVE = './logs/'\n","if not os.path.exists(PATH_SAVE):\n","       os.makedirs(PATH_SAVE)\n","for epoch in range(20):\n","  for i1, i2 in zip(enumerate(train_loader1),enumerate(train_loader2)):\n","    dem = dem + 1\n","    stt1, data1 = i1\n","    input1,target1 = data1\n","\n","    stt2, data2 = i2\n","    input2,target2 = data2\n","\n","    optimizer.zero_grad()\n","    input1 = input1.cuda()\n","    input2 = input2.cuda()\n","\n","    outputs = x_model(input1,input2)\n","    outputs = outputs.view(1,8)\n","\n","    loss = criterion(outputs, target1)\n","    loss.backward()\n","    optimizer.step()\n","torch.save(x_model.state_dict(), PATH_SAVE + 'final.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eEdhJ36bw_i2","colab_type":"text"},"source":["**TEST CONCATE CLAHE VESSEL**"]},{"cell_type":"code","metadata":{"id":"jQxMjTB8w_E4","colab_type":"code","colab":{}},"source":["test_data = pd.read_csv('./utils/XYZ_ODIR.csv')\n","test_data_left = test_data.copy()\n","test_data_right = test_data.copy()\n","test_data_left.loc[:, 'id'] = test_data_left.ID.apply(\n","    lambda x: str(x)+'_left.jpg')\n","test_data_right.loc[:, 'id'] = test_data_left.ID.apply(\n","    lambda x: str(x)+'_right.jpg')\n","test_data = pd.concat([test_data_left, test_data_right])\n","test_data.sort_values(['ID'], inplace=True)\n","\n","test_path1 = './data/clahe-test/'\n","test_dataset_clahe = EyeDataset(dataset_path=test_path1,\n","                                labels=test_data.loc[:, labels].values,\n","                                ids=test_data.loc[:, 'id'].values,\n","                                albumentations_tr=aug_val(image_size))\n","test_loader_clahe = DataLoader(test_dataset_clahe,\n","                                num_workers=8,\n","                                pin_memory=False,\n","                                batch_size=batch_size,\n","                                shuffle=False)\n","\n","test_path2 = './data/vessel-test/'\n","test_dataset_vessel = EyeDataset(dataset_path=test_path2,\n","                                labels=test_data.loc[:, labels].values,\n","                                ids=test_data.loc[:, 'id'].values,\n","                                albumentations_tr=aug_val(image_size))\n","test_loader_vessel = DataLoader(test_dataset_vessel,\n","                                num_workers=8,\n","                                pin_memory=False,\n","                                batch_size=batch_size,\n","                                shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IHB4xl_uxcQ0","colab_type":"code","colab":{}},"source":["x_model = NeuralNet(modelA,modelB)\n","x_model.load_state_dict(torch.load(os.path.join('./logs/final.pth')))\n","x_model.eval()\n","for i1, i2 in zip(enumerate(test_loader_clahe), enumerate(test_loader_vessel)):\n","    stt1, data1 = i1\n","    input1, target1 = data1\n","    stt2, data2 = i2\n","    input2, target2 = data2\n","    input1 = input1.cuda()\n","    input2 = input2.cuda()\n","\n","    outputs = x_model(input1, input2)\n","    outputs = torch.Tensor.cpu(outputs).detach().numpy()\n","    probabilities = softmax(torch.from_numpy(outputs), dim=0).numpy()\n","    for idx in range(probabilities.shape[0]):\n","        if all(probabilities[:] < 0.5):\n","            probabilities[0] = 1.0\n","    probabilities_list.append(probabilities)\n","arr_list = []\n","arr = np.array(probabilities_list)\n","arr_list.append(arr)\n","probabilities_combined = np.stack(arr_list, axis=0).mean(axis=0)\n","predicted_labels = pd.DataFrame(probabilities_combined, columns=labels)\n","predicted_labels['id'] = test_data.loc[:, 'id'].values\n","predicted_labels.loc[:, 'ID'] = predicted_labels.id.apply(\n","    lambda x: x.split('_')[0])\n","predicted_labels_groupped = predicted_labels.groupby(\n","    ['ID']).aggregate(dict(zip(labels, ['max']*(len(labels)))))\n","print(type(predicted_labels_groupped))\n","predicted_labels_groupped['ID'] = predicted_labels_groupped.index.values.astype(\n","    int)\n","predicted_labels_groupped.reset_index(drop=True, inplace=True)\n","predicted_labels_groupped.sort_values('ID', inplace=True)\n","predicted_labels_groupped = predicted_labels_groupped.loc[:, ['ID']+labels]\n","predicted_labels_groupped.to_csv('./submit/final.csv', index=False)"],"execution_count":null,"outputs":[]}]}